{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import zipfile\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "import ast"
      ],
      "metadata": {
        "id": "iYZVY2BwNdYA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Base path to interested drive**"
      ],
      "metadata": {
        "id": "Cy1R57Y1PzeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/dataDir\"\n",
        "USER_PATH = BASE_PATH + \"/users\"\n",
        "MANGA_PATH = BASE_PATH + \"/manga\"\n",
        "ANIME_PATH = BASE_PATH + \"/anime\"\n",
        "ANIME_REV_PATH = ANIME_PATH + \"/anime_rev\"\n",
        "MANGA_REV_PATH = MANGA_PATH + \"/manga_rev\"\n",
        "ANIME_SCORES_PATH = ANIME_PATH + \"/anime_scores\"\n",
        "MANGA_SCORES_PATH = MANGA_PATH + \"/manga_scores\"\n",
        "MONGO_PATH = BASE_PATH + \"/mongo_csv\"\n",
        "NEO_PATH = BASE_PATH + \"/neo4j_csv\""
      ],
      "metadata": {
        "id": "0AeXs-2EbH9W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLUBS"
      ],
      "metadata": {
        "id": "3Fe2Xa7bwTUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting users from same clubs allow to get slients with similar tastes, creating an interesting and connected database"
      ],
      "metadata": {
        "id": "EyaanC7TNjie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Clubs from first 9 pages of web site with at least 30 members**"
      ],
      "metadata": {
        "id": "P79aNAJmP3-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_number(string):\n",
        "    return int(string.strip().replace(\",\", \"\"))\n",
        "\n",
        "\n",
        "clubs_id = set()\n",
        "possibles_users = 0\n",
        "page = 1\n",
        "\n",
        "for i in range(1,9):\n",
        "    print(f\"\\r{page}\", end=\"\")\n",
        "\n",
        "    time.sleep(3)  # Wait 3 seconds per page\n",
        "    data = requests.get(f\"https://myanimelist.net/clubs.php?p={page}\")\n",
        "    soup = BeautifulSoup(data.text, \"html.parser\")\n",
        "    rows = soup.find_all(\"tr\", {\"class\": \"table-data\"})\n",
        "    for row in rows:\n",
        "        members = get_number(row.find(\"td\", {\"class\": \"ac\"}).text)\n",
        "        club_id = get_number(\n",
        "            row.find(\"a\", {\"class\": \"fw-b\"}).get(\"href\").split(\"=\")[-1]\n",
        "        )\n",
        "        if (\n",
        "            club_id not in clubs_id and members > 30\n",
        "        ):  # Only save groups with more than 30 members\n",
        "            possibles_users += members\n",
        "            clubs_id.add(club_id)\n",
        "\n",
        "    page += 1\n",
        "    if possibles_users > 1000000:  # Threshold to stop\n",
        "        break\n",
        "\n",
        "with open(f\"{BASE_PATH}/clubs.txt\", \"w\") as file:\n",
        "    for club in clubs_id:\n",
        "        file.write(f\"{club}\\n\")"
      ],
      "metadata": {
        "id": "mM5rknvvchwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Creating and opening file to retrieve usernames***"
      ],
      "metadata": {
        "id": "Er_QuML0QFz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f\"{BASE_PATH}/users_list.txt\"):\n",
        "    with open(f\"{BASE_PATH}/users_list.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "        pass\n",
        "\n",
        "if not os.path.exists(f\"{BASE_PATH}/_revised_clubs.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_revised_clubs.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "        pass"
      ],
      "metadata": {
        "id": "7ESXJv_GepVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{BASE_PATH}/clubs.txt\") as file:\n",
        "    clubs_id = [x.strip() for x in file.readlines()]\n",
        "\n",
        "with open(f\"{BASE_PATH}/users_list.txt\", encoding=\"UTF-8\") as file:\n",
        "    users = set([x.strip() for x in file.readlines()])\n",
        "\n",
        "with open(f\"{BASE_PATH}/_revised_clubs.txt\", encoding=\"UTF-8\") as file:\n",
        "    revised_clubs = set([int(x.strip()) for x in file.readlines()])\n",
        "\n",
        "len(users), len(revised_clubs), len(clubs_id)"
      ],
      "metadata": {
        "id": "G7JwFBOFetfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##USERS"
      ],
      "metadata": {
        "id": "KgcgcUHdq8fX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieving usernames in clubs with JikanAPI**"
      ],
      "metadata": {
        "id": "hspOB48UQNwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, club_id in enumerate(clubs_id):\n",
        "    if club_id in revised_clubs:\n",
        "        continue\n",
        "\n",
        "    page = 1\n",
        "    while True:\n",
        "        print(f\"\\r{i+1}/{len(clubs_id)} --> {str(page).zfill(2)}\", end=\"\")\n",
        "        link = f\"https://api.jikan.moe/v4/clubs/{club_id}/members\"\n",
        "\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            data = requests.get(link,f\"page={page}\")\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt()\n",
        "        except:  # Other exception wait 2 min and try again\n",
        "            time.sleep(120)\n",
        "            continue\n",
        "\n",
        "        if data.status_code != 200:\n",
        "            break\n",
        "\n",
        "        with open(f\"{BASE_PATH}/users_list.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "            for user in map(lambda x: x[\"username\"], json.loads(data.text)[\"data\"]):\n",
        "                if user not in users and user != \"\":\n",
        "                    file.write(f\"{user}\\n\")\n",
        "                    users.add(user)\n",
        "        page += 1\n",
        "\n",
        "    revised_clubs.add(club_id)\n",
        "    with open(f\"{BASE_PATH}/_revised_clubs.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{club_id}\\n\")"
      ],
      "metadata": {
        "id": "zKJupbtue4Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Shuffling username and giving them new ids, creating users.csv***"
      ],
      "metadata": {
        "id": "-1zqyJUOQaC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{BASE_PATH}/users_list.txt\", encoding=\"UTF-8\") as file:\n",
        "    users = list(set([x.strip() for x in file.readlines()]))[1:]\n",
        "    random.shuffle(users)\n",
        "\n",
        "with open(f\"{BASE_PATH}/users.csv\", \"w\", encoding=\"UTF-8\") as file:\n",
        "    file.write(\"user_id,username\\n\")\n",
        "    for i, user in enumerate(users):\n",
        "        file.write(f\"{i},{user}\\n\")"
      ],
      "metadata": {
        "id": "OQ3rooWSjm8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MANGA"
      ],
      "metadata": {
        "id": "sDU_SHe_260H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using MAL API**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gNLA4c3npQ5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **For each Username retrieving his manga**\n",
        "gotten data: manga_id, manga_title, status, chapters_read, score"
      ],
      "metadata": {
        "id": "5n_9rOgSx8RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{BASE_PATH}/users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    users = [(int(x[0]), x[1]) for x in users]\n",
        "\n",
        "last_revised_users = -1\n",
        "if os.path.exists(f\"{BASE_PATH}/_manga_revised_users.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_manga_revised_users.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
        "        last_revised_users = int(file.readlines()[-1])\n",
        "\n",
        "len(users), last_revised_users"
      ],
      "metadata": {
        "id": "CcuST9GAPx_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = {\n",
        "    'X-MAL-CLIENT-ID': '',\n",
        "}\n",
        "for i, (user_id, username) in enumerate(users):\n",
        "    if user_id <= last_revised_users:\n",
        "        continue\n",
        "\n",
        "    now = datetime.now()\n",
        "    print(f'\\r{str(now).split(\".\")[0]} --> {i+1}/{len(users)}', end=\"\")\n",
        "    offset = 0\n",
        "    all_manga = []\n",
        "\n",
        "    while True:\n",
        "        link = f\"https://api.myanimelist.net/v2/users/{username}/mangalist?offset={offset}&fields=list_status&limit=300\"\n",
        "        try:\n",
        "            time.sleep(0.4)\n",
        "            data = requests.get(link, headers = header, timeout=15)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt()\n",
        "        except:  # Other exception wait 2 min and try again\n",
        "            time.sleep(120)\n",
        "            continue\n",
        "\n",
        "        if data.status_code != 200:\n",
        "            break\n",
        "\n",
        "        payload = json.loads(data.text)[\"data\"]\n",
        "        for manga in payload:\n",
        "            all_manga.append((manga[\"node\"][\"id\"], manga[\"node\"][\"title\"].replace(',', '..'), manga[\"list_status\"][\"status\"], manga[\"list_status\"][\"num_chapters_read\"], manga[\"list_status\"][\"score\"]))\n",
        "\n",
        "        offset += 300\n",
        "        if len(payload) < 300:\n",
        "            break\n",
        "\n",
        "    if len(all_manga) != 0:\n",
        "        if not os.path.exists(f\"{USER_PATH}/user_{user_id}\"):\n",
        "          os.makedirs(f\"{USER_PATH}/user_{user_id}\", exist_ok=True)\n",
        "        with open(f\"{USER_PATH}/user_{user_id}/user_{user_id}_manga_list.csv\", \"w\") as f1:\n",
        "            f1.write(f\"manga_id,manga_title,status,read_chapters,score\\n\")\n",
        "            for manga_id, manga_title, status, read_chapters,score in all_manga:\n",
        "                f1.write(\n",
        "                    f\"{manga_id},{manga_title},{status},{read_chapters},{score}\\n\"\n",
        "                )\n",
        "\n",
        "    last_revised_users = user_id\n",
        "    with open(f\"{BASE_PATH}/_manga_revised_users.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{user_id}\\n\")"
      ],
      "metadata": {
        "id": "eeXTXfbJT8AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving users for whom I have actually retrieved the lists**"
      ],
      "metadata": {
        "id": "YjjViDYOXzOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{BASE_PATH}/users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    users = [(int(x[0]), x[1]) for x in users]\n",
        "\n",
        "last_revised_users = -1\n",
        "if os.path.exists(f\"{BASE_PATH}/_manga_revised_users.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_manga_revised_users.txt\", \"r\", encoding=\"UTF-8\") as f1:\n",
        "        last_revised_users = int(f1.readlines()[-1])\n",
        "\n",
        "with open(f'{BASE_PATH}/manga_users.csv', 'w') as f2:\n",
        "   for user_id, username in users:\n",
        "      if user_id <= last_revised_users:\n",
        "         f2.write(f'{user_id},{username}\\n')\n",
        "         print(user_id)"
      ],
      "metadata": {
        "id": "O1e27zWOVP4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating manga list based on users lists content"
      ],
      "metadata": {
        "id": "kqNJVSxd3b2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_manga = set() # il set consente solo valori unici\n",
        "folders = os.listdir(USER_PATH)\n",
        "for i, user_dir in enumerate(folders):\n",
        "    folder_content = os.listdir(f\"{USER_PATH}/{user_dir}\")\n",
        "    for j, user_file in enumerate(folder_content):\n",
        "      if \"manga_list.csv\" not in user_file:\n",
        "          continue\n",
        "\n",
        "      print(f\"\\r{i + 1}/{len(folders)} -> file {j+1}\", end=\"\")\n",
        "      with open(f\"{USER_PATH}/{user_dir}/{user_file}\", \"r\") as file:\n",
        "          file.readline()\n",
        "          for line in file:\n",
        "              manga = line.strip().split(\",\")[0]\n",
        "              unique_manga.add(manga)\n",
        "\n",
        "print(\"         \")\n",
        "print(len(unique_manga))"
      ],
      "metadata": {
        "id": "puqZy50U3cXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f\"{BASE_PATH}/_revised_manga.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_revised_manga.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "        pass\n",
        "\n",
        "with open(f\"{BASE_PATH}/_revised_manga.txt\", encoding=\"UTF-8\") as file:\n",
        "  revised_manga = set([int(x.strip()) for x in file.readlines()])\n",
        "\n",
        "len(unique_manga), len(revised_manga)"
      ],
      "metadata": {
        "id": "FDFWqFcokV3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_manga_detail(text_data, field):\n",
        "  content=json.loads(text_data)\n",
        "  if field in content:\n",
        "    if field == \"genres\":\n",
        "      genres_list = []\n",
        "      for genre in content[field]:\n",
        "        genres_list.append(genre['name'])\n",
        "      genres = \"-\".join(genres_list)\n",
        "      return genres\n",
        "    elif field == \"authors\":\n",
        "      author_list = []\n",
        "      for author in content[field]:\n",
        "        name = \"\"\n",
        "        if \"first_name\" in author['node']:\n",
        "          name = author['node']['first_name']\n",
        "        if \"last_name\" in author['node']:\n",
        "          name = name + \"+\"+ author['node']['last_name']\n",
        "        author_list.append(name)\n",
        "      authors = \"-\".join(author_list)\n",
        "      return authors\n",
        "    elif field == \"title\" or field == \"synopsis\":\n",
        "      comma_removed_field = content[field].replace(\",\", \"....\")\n",
        "      if field == \"synopsis\":\n",
        "        comma_removed_field = comma_removed_field.replace(\"\\n\", \"__\")\n",
        "      return comma_removed_field\n",
        "    else:\n",
        "      return content[field]\n",
        "  else:\n",
        "    return \"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "VlCcSuxa2dOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting manga details for each manga found in users lists"
      ],
      "metadata": {
        "id": "aLagESvlc12q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "header = {\n",
        "    'X-MAL-CLIENT-ID': '',\n",
        "}\n",
        "get_fields = [\"title\", \"status\", \"media_type\", \"start_date\", \"num_chapters\", \"authors{first_name, last_name}\", \"genres\", \"synopsis\", \"rank\", \"mean\"]\n",
        "used_fields = [\"title\", \"status\", \"media_type\", \"start_date\", \"num_chapters\", \"authors\", \"genres\", \"synopsis\", \"rank\", \"mean\"]\n",
        "get_query = f\"?fields={','.join(get_fields)}\"\n",
        "\n",
        "skipped = 0\n",
        "for i, manga_id in enumerate(unique_manga):\n",
        "    if int(manga_id) in revised_manga:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    now = datetime.now()\n",
        "    print(f'\\r{str(now).split(\".\")[0]} --> {i+1}/{len(unique_manga)} ({manga_id}), skip = {skipped}', end=\"\")\n",
        "    all_details = []\n",
        "\n",
        "    while True:\n",
        "        link = f\"https://api.myanimelist.net/v2/manga/{manga_id}{get_query}\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            data = requests.get(link, headers = header, timeout=5)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt()\n",
        "        except:  # Other exception wait 1.5 min and try again\n",
        "            time.sleep(90)\n",
        "            continue\n",
        "\n",
        "        if data.status_code != 200:\n",
        "            break\n",
        "\n",
        "        payload = json.loads(data.text)\n",
        "        for field in used_fields:\n",
        "          all_details.append(get_manga_detail(data.text, field))\n",
        "        break\n",
        "\n",
        "    if len(all_details) != 0:\n",
        "        manga_row = \",\".join(str(det) for det in all_details)\n",
        "        if not os.path.exists(f\"{MANGA_PATH}/manga_list.csv\"):\n",
        "          with open(f\"{MANGA_PATH}/manga_list.csv\", \"w\") as f1:\n",
        "            f1.write(f\"manga_id,manga_title,status,media_type,start_date,num_chapters,authors,genres,synopsis,rank,mean\\n\")\n",
        "            f1.close()\n",
        "        with open(f\"{MANGA_PATH}/manga_list.csv\", \"a\") as f2:\n",
        "          f2.write(f\"{manga_id},{manga_row}\\n\")\n",
        "\n",
        "    revised_manga.add(manga_id)\n",
        "    with open(f\"{BASE_PATH}/_revised_manga.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{manga_id}\\n\")"
      ],
      "metadata": {
        "id": "nYJhN62qZUAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{MANGA_PATH}/manga_list.csv\", \"r\") as f:\n",
        "    righe = f.readlines()\n",
        "\n",
        "righe_senza_duplicati = list(set(righe))\n",
        "len(righe), len(righe_senza_duplicati), len(righe) - len(righe_senza_duplicati)\n",
        "\n",
        "## Scrivere le righe uniche in un nuovo file\n",
        "#with open(f\"{BASE_PATH}/_revised_manga.txt\", \"w\") as f:\n",
        "#    f.writelines(righe_senza_duplicati)"
      ],
      "metadata": {
        "id": "RgT93I1CqXmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANIME"
      ],
      "metadata": {
        "id": "3KPZTcZGoCIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **For each Username retrieving his anime lists**\n",
        "gotten data: anime_id, anime_title, status, episodes_watched score"
      ],
      "metadata": {
        "id": "e7BRTtZUxY6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{BASE_PATH}/users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    users = [(int(x[0]), x[1]) for x in users]\n",
        "\n",
        "last_revised_users = -1\n",
        "if os.path.exists(f\"{BASE_PATH}/_anime_revised_users.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_anime_revised_users.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
        "        last_revised_users = int(file.readlines()[-1])\n",
        "\n",
        "last_revised_manga_users = -1\n",
        "if os.path.exists(f\"{BASE_PATH}/_manga_revised_users.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_manga_revised_users.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
        "        last_revised_manga_users = int(file.readlines()[-1])\n",
        "\n",
        "len(users), last_revised_users, last_revised_manga_users"
      ],
      "metadata": {
        "id": "dCohBAwBob2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = {\n",
        "    'X-MAL-CLIENT-ID': '',\n",
        "}\n",
        "for i, (user_id, username) in enumerate(users):\n",
        "    if user_id <= last_revised_users:\n",
        "        continue\n",
        "    if user_id > (last_revised_manga_users + 1000):\n",
        "        break\n",
        "\n",
        "    now = datetime.now()\n",
        "    print(f'\\r{str(now).split(\".\")[0]} --> {i+1}/{len(users)}', end=\"\")\n",
        "    offset = 0\n",
        "    all_anime = []\n",
        "\n",
        "    while True:\n",
        "        link = f\"https://api.myanimelist.net/v2/users/{username}/animelist?offset={offset}&fields=list_status&limit=300\"\n",
        "        try:\n",
        "            time.sleep(0.4)\n",
        "            data = requests.get(link, headers = header, timeout=15)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt()\n",
        "        except:  # Other exception wait 1.5 min and try again\n",
        "            time.sleep(90)\n",
        "            continue\n",
        "\n",
        "        if data.status_code != 200:\n",
        "            break\n",
        "\n",
        "        payload = json.loads(data.text)[\"data\"]\n",
        "        for anime in payload:\n",
        "            all_anime.append((anime[\"node\"][\"id\"], anime[\"node\"][\"title\"].replace(',', '..'), anime[\"list_status\"][\"status\"], anime[\"list_status\"][\"num_episodes_watched\"], anime[\"list_status\"][\"score\"]))\n",
        "\n",
        "        offset += 300\n",
        "        if len(payload) < 300:\n",
        "            break\n",
        "\n",
        "    if len(all_anime) != 0:\n",
        "        if not os.path.exists(f\"{USER_PATH}/user_{user_id}\"):\n",
        "          os.makedirs(f\"{USER_PATH}/user_{user_id}\", exist_ok=True)\n",
        "        with open(f\"{USER_PATH}/user_{user_id}/user_{user_id}_anime_list.csv\", \"w\") as f1:\n",
        "            f1.write(f\"anime_id,anime_title,status,watched_episodes,score\\n\")\n",
        "            for anime_id,anime_title,status,watched_episodes,score in all_anime:\n",
        "                f1.write(\n",
        "                    f\"{anime_id},{anime_title},{status},{watched_episodes},{score}\\n\"\n",
        "                )\n",
        "\n",
        "    last_revised_users = user_id\n",
        "    with open(f\"{BASE_PATH}/_anime_revised_users.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{user_id}\\n\")"
      ],
      "metadata": {
        "id": "UXtYYClIoGiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{BASE_PATH}/users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    users = [(int(x[0]), x[1]) for x in users]\n",
        "\n",
        "last_revised_users = -1\n",
        "if os.path.exists(f\"{BASE_PATH}/_anime_revised_users.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_anime_revised_users.txt\", \"r\", encoding=\"UTF-8\") as f1:\n",
        "        last_revised_users = int(f1.readlines()[-1])\n",
        "\n",
        "with open(f'{BASE_PATH}/anime_users.csv', 'w') as f2:\n",
        "   for user_id, username in users:\n",
        "      if user_id <= last_revised_users:\n",
        "         f2.write(f'{user_id},{username}\\n')\n",
        "         print(user_id)"
      ],
      "metadata": {
        "id": "n6CY_3YwtAb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating anime list based on users lists content"
      ],
      "metadata": {
        "id": "T3dgKg5hxhW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_anime = set() # il set consente solo valori unici\n",
        "folders = os.listdir(USER_PATH)\n",
        "for i, user_dir in enumerate(folders):\n",
        "    folder_content = os.listdir(f\"{USER_PATH}/{user_dir}\")\n",
        "    for j, user_file in enumerate(folder_content):\n",
        "      if \"anime_list.csv\" not in user_file:\n",
        "          continue\n",
        "\n",
        "      print(f\"\\r{i + 1}/{len(folders)} -> file {j+1}\", end=\"\")\n",
        "      with open(f\"{USER_PATH}/{user_dir}/{user_file}\", \"r\") as file:\n",
        "          file.readline()\n",
        "          for line in file:\n",
        "              anime = line.strip().split(\",\")[0]\n",
        "              unique_anime.add(anime)\n",
        "\n",
        "print(\"         \")\n",
        "print(len(unique_anime))"
      ],
      "metadata": {
        "id": "NkQYfXrhxsg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f\"{BASE_PATH}/_revised_anime.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_revised_anime.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "        pass\n",
        "\n",
        "with open(f\"{BASE_PATH}/_revised_anime.txt\", encoding=\"UTF-8\") as file:\n",
        "  revised_anime = set([int(x.strip()) for x in file.readlines()])\n",
        "\n",
        "len(unique_anime), len(revised_anime)"
      ],
      "metadata": {
        "id": "7ujW9MNGya9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_anime_detail(text_data, field):\n",
        "  content=json.loads(text_data)\n",
        "  if field in content:\n",
        "    if field == \"genres\":\n",
        "      genres_list = []\n",
        "      for genre in content[field]:\n",
        "        genres_list.append(genre['name'])\n",
        "      genres = \"-\".join(genres_list)\n",
        "      return genres\n",
        "    elif field == \"studios\":\n",
        "      studios_list = []\n",
        "      for studio in content[field]:\n",
        "          studios_list.append(studio['name'].replace(',', '....'))\n",
        "      studios = \"-\".join(studios_list)\n",
        "      return studios\n",
        "    elif field == \"title\" or field == \"synopsis\":\n",
        "      comma_removed_field = content[field].replace(\",\", \"....\")\n",
        "      if field == \"synopsis\":\n",
        "        comma_removed_field = comma_removed_field.replace(\"\\n\", \"__\")\n",
        "      return comma_removed_field\n",
        "    else:\n",
        "      return content[field]\n",
        "  else:\n",
        "    return \"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "HO-mAcCeytmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = {\n",
        "    'X-MAL-CLIENT-ID': '',\n",
        "}\n",
        "get_fields = [\"title\", \"status\", \"media_type\", \"start_date\", \"end_date\", \"num_episodes\", \"average_episode_duration\", \"studios\", \"source\", \"genres\", \"synopsis\", \"rank\", \"mean\"]\n",
        "get_query = f\"?fields={','.join(get_fields)}\"\n",
        "\n",
        "skipped = 0\n",
        "for i, anime_id in enumerate(unique_anime):\n",
        "    if int(anime_id) in revised_anime:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    now = datetime.now()\n",
        "    print(f'\\r{str(now).split(\".\")[0]} --> {i+1}/{len(unique_anime)} ({anime_id}), skip = {skipped}', end=\"\")\n",
        "    all_details = []\n",
        "\n",
        "    while True:\n",
        "        link = f\"https://api.myanimelist.net/v2/anime/{anime_id}{get_query}\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            data = requests.get(link, headers = header, timeout=5)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt()\n",
        "        except:  # Other exception wait 1.5 min and try again\n",
        "            time.sleep(90)\n",
        "            continue\n",
        "\n",
        "        if data.status_code != 200:\n",
        "            break\n",
        "\n",
        "        payload = json.loads(data.text)\n",
        "        for field in get_fields:\n",
        "          all_details.append(get_anime_detail(data.text, field))\n",
        "        break\n",
        "\n",
        "    if len(all_details) != 0:\n",
        "        anime_row = \",\".join(str(det) for det in all_details)\n",
        "        if not os.path.exists(f\"{ANIME_PATH}/anime_list.csv\"):\n",
        "          with open(f\"{ANIME_PATH}/anime_list.csv\", \"w\") as f1:\n",
        "            f1.write(f\"anime_id,anime_title,status,media_type,start_date, end_date,num_episodes,average_ep_duration_sec,studios,source, genres,synopsis,rank,mean\\n\")\n",
        "            f1.close()\n",
        "        with open(f\"{ANIME_PATH}/anime_list.csv\", \"a\") as f2:\n",
        "          f2.write(f\"{anime_id},{anime_row}\\n\")\n",
        "\n",
        "    revised_anime.add(anime_id)\n",
        "    with open(f\"{BASE_PATH}/_revised_anime.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{anime_id}\\n\")"
      ],
      "metadata": {
        "id": "XWsSuQkLz2dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#JOINING DATA"
      ],
      "metadata": {
        "id": "eIJaZiexrDeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating a list joining users used for manga and anime retrieval"
      ],
      "metadata": {
        "id": "Xvs-v07m1BG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{BASE_PATH}/manga_users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    manga_users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    manga_users = [(int(x[0]), x[1]) for x in manga_users]\n",
        "with open(f\"{BASE_PATH}/anime_users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    anime_users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    anime_users = [(int(x[0]), x[1]) for x in anime_users]\n",
        "\n",
        "if not os.path.exists(f\"{BASE_PATH}/_revised_join_users.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_revised_join_users.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "        pass\n",
        "\n",
        "with open(f\"{BASE_PATH}/_revised_join_users.txt\", encoding=\"UTF-8\") as file:\n",
        "  revised_join_users = set([x.strip() for x in file.readlines()])\n",
        "\n",
        "print(len(revised_join_users), len(manga_users), len(anime_users))"
      ],
      "metadata": {
        "id": "0xDMf4tLrJKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding users from which I've taken the manga list to the joined users list"
      ],
      "metadata": {
        "id": "6LSHVCrK07VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_data(text_data, field):\n",
        "  content=json.loads(text_data)[\"data\"]\n",
        "  if field in content:\n",
        "    if field == \"location\" and content[field] != None:\n",
        "        return content[field].replace(\",\", \"-\")\n",
        "    return content[field]\n",
        "  else:\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "2NrSHQvu5yyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieving user information"
      ],
      "metadata": {
        "id": "xWXjymXy1X8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_field = [\"birthday\", \"location\", \"joined\"]\n",
        "skipped = 0\n",
        "for i, (user_id, username) in enumerate(anime_users):\n",
        "    if username in revised_join_users:\n",
        "        skipped += 1\n",
        "        continue\n",
        "    if not os.path.exists(f\"{USER_PATH}/user_{user_id}/user_{user_id}_anime_list.csv\"):\n",
        "        continue\n",
        "\n",
        "\n",
        "    now = datetime.now()\n",
        "    print(f'\\r{str(now).split(\".\")[0]} --> {i+1}/{len(anime_users)} ({username}), skip = {skipped}', end=\"\")\n",
        "    all_user_data = []\n",
        "\n",
        "    while True:\n",
        "        link = f\"https://api.jikan.moe/v4/users/{username}\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            data = requests.get(link, timeout=5)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt()\n",
        "        except:  # Other exception wait 1.5 min and try again\n",
        "            time.sleep(90)\n",
        "            continue\n",
        "\n",
        "        if data.status_code != 200:\n",
        "            break\n",
        "\n",
        "        payload = json.loads(data.text)\n",
        "        for field in user_field:\n",
        "          all_user_data.append(get_user_data(data.text, field))\n",
        "        break\n",
        "    if len(all_user_data) != 0:\n",
        "        user_row = \",\".join(str(det) for det in all_user_data)\n",
        "        if not os.path.exists(f\"{BASE_PATH}/joined_users.csv\"):\n",
        "          with open(f\"{BASE_PATH}/joined_users.csv\", \"w\") as f1:\n",
        "            f1.write(f\"user_id,username,birthday,location,joined_at\\n\")\n",
        "            f1.close()\n",
        "        with open(f\"{BASE_PATH}/joined_users.csv\", \"a\") as f2:\n",
        "          f2.write(f\"{user_id},{username},{user_row}\\n\")\n",
        "    revised_join_users.add(username)\n",
        "    with open(f\"{BASE_PATH}/_revised_join_users.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{username}\\n\")\n"
      ],
      "metadata": {
        "id": "d38aIM8W64ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REVIEWS"
      ],
      "metadata": {
        "id": "nHYPOyfmJPQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_text(rev):\n",
        "  rev = rev.replace(\",\", \"....\")\n",
        "  rev = rev.replace(\"\\n\", \"|\")\n",
        "  rev = rev.replace(\"\\n\\n\", \"||\")\n",
        "  rev = rev.replace(\"\\r\", \" \")\n",
        "  return rev\n"
      ],
      "metadata": {
        "id": "fLTHX6jyJOyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANIME REVIEWS"
      ],
      "metadata": {
        "id": "xBwSRINYJUbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recovering all users and anime retrieved"
      ],
      "metadata": {
        "id": "IXZ27qY7JjYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f\"{ANIME_PATH}/anime_rev\"):\n",
        "          os.makedirs(f\"{ANIME_PATH}/anime_rev\", exist_ok=True)\n",
        "\n",
        "with open(f\"{BASE_PATH}/joined_users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    all_users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    all_users = [x[1] for x in all_users] # Retrieving usernames to get reviwes only of saved users\n",
        "with open(f\"{ANIME_PATH}/anime_list.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    all_anime = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    all_anime = [(int(x[0]), x[1]) for x in all_anime] # Retrieving usernames to get reviwes only of saved anime\n",
        "\n",
        "if not os.path.exists(f\"{BASE_PATH}/_revised_anime_rev.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_revised_anime_rev.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "        pass\n",
        "\n",
        "with open(f\"{BASE_PATH}/_revised_anime_rev.txt\", encoding=\"UTF-8\") as file:\n",
        "  revised_anime_rev = set([int(x.strip()) for x in file.readlines()])\n",
        "\n",
        "print(len(revised_anime_rev), len(all_anime), len(all_users))"
      ],
      "metadata": {
        "id": "Hl4JEYjnJi_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scraping anime review for anime in our db and user registered"
      ],
      "metadata": {
        "id": "swAd7OUvHddj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skipped = 0\n",
        "for i, (anime_id, anime_title) in enumerate(all_anime):\n",
        "    if anime_id in revised_anime_rev:\n",
        "        skipped += 1\n",
        "        continue\n",
        "    all_anime_reviews = []\n",
        "\n",
        "    page = 1\n",
        "    while True:\n",
        "        print(f\"\\r{i+1}/{len(all_anime)} --> {str(page).zfill(2)}, anime: {anime_title}  (skipped = {skipped})\", end=\"\")\n",
        "        link = f\"https://api.jikan.moe/v4/anime/{anime_id}/reviews\"\n",
        "\n",
        "        try:\n",
        "            time.sleep(0.6)\n",
        "            data = requests.get(link,f\"page={page}\")\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt()\n",
        "        except:  # Other exception wait 1.5 min and try again\n",
        "            time.sleep(90)\n",
        "            continue\n",
        "\n",
        "        if data.status_code != 200:\n",
        "            break\n",
        "\n",
        "        payload = json.loads(data.text)[\"data\"]\n",
        "        for review in payload:\n",
        "            if review[\"user\"][\"username\"] in all_users:\n",
        "              all_anime_reviews.append((review[\"user\"][\"username\"], review[\"score\"], prepare_text(review[\"review\"]), review[\"date\"]))\n",
        "        if len(payload) < 20:\n",
        "            break\n",
        "        page += 1\n",
        "\n",
        "    if len(all_anime_reviews) != 0:\n",
        "        with open(f\"{ANIME_REV_PATH}/reviews_{anime_id}.csv\", \"w\") as f1:\n",
        "            f1.write(f\"username,score,review,date\\n\")\n",
        "            for user, score, review, date in all_anime_reviews:\n",
        "                f1.write(\n",
        "                    f\"{user},{score},{review},{date}\\n\"\n",
        "                )\n",
        "\n",
        "    revised_anime_rev.add(anime_id)\n",
        "    with open(f\"{BASE_PATH}/_revised_anime_rev.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{anime_id}\\n\")"
      ],
      "metadata": {
        "id": "CyEcNkf1LG4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MANGA REVIEWS"
      ],
      "metadata": {
        "id": "2ZOTksf5HRkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recovering all users and manga scraped"
      ],
      "metadata": {
        "id": "SbVB5ihzHYvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f\"{MANGA_PATH}/manga_rev\"):\n",
        "          os.makedirs(f\"{MANGA_PATH}/manga_rev\", exist_ok=True)\n",
        "\n",
        "with open(f\"{BASE_PATH}/joined_users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    all_users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    all_users = [x[1] for x in all_users] # Retrieving usernames to get reviwes only of saved users\n",
        "with open(f\"{MANGA_PATH}/manga_list.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    all_manga = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    all_manga = [(int(x[0]), x[1]) for x in all_manga] # Retrieving usernames to get reviwes only of saved manga\n",
        "\n",
        "if not os.path.exists(f\"{BASE_PATH}/_revised_manga_rev.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_revised_manga_rev.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "        pass\n",
        "\n",
        "with open(f\"{BASE_PATH}/_revised_manga_rev.txt\", encoding=\"UTF-8\") as file:\n",
        "  revised_manga_rev = set([int(x.strip()) for x in file.readlines()])\n",
        "\n",
        "print(len(revised_manga_rev), len(all_manga), len(all_users))"
      ],
      "metadata": {
        "id": "e2U13LnEHo-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scraping anime review for manga in our db and user registered"
      ],
      "metadata": {
        "id": "Z51fyzV4Hne1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skipped = 0\n",
        "for i, (manga_id, manga_title) in enumerate(all_manga):\n",
        "    if manga_id in revised_manga_rev:\n",
        "        skipped += 1\n",
        "        continue\n",
        "    all_manga_reviews = []\n",
        "\n",
        "    page = 1\n",
        "    while True:\n",
        "        print(f\"\\r{i+1}/{len(all_manga)} --> {str(page).zfill(2)}, manga: {manga_title}  (skipped = {skipped})\", end=\"\")\n",
        "        link = f\"https://api.jikan.moe/v4/manga/{manga_id}/reviews\"\n",
        "\n",
        "        try:\n",
        "            time.sleep(0.6)\n",
        "            data = requests.get(link,f\"page={page}\")\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt()\n",
        "        except:  # Other exception wait 1.5 min and try again\n",
        "            time.sleep(90)\n",
        "            continue\n",
        "\n",
        "        if data.status_code != 200:\n",
        "            break\n",
        "\n",
        "        payload = json.loads(data.text)[\"data\"]\n",
        "        for review in payload:\n",
        "            if review[\"user\"][\"username\"] in all_users:\n",
        "              all_manga_reviews.append((review[\"user\"][\"username\"], review[\"score\"], prepare_text(review[\"review\"]), review[\"date\"]))\n",
        "        if len(payload) < 20:\n",
        "            break\n",
        "        page += 1\n",
        "\n",
        "    if len(all_manga_reviews) != 0:\n",
        "        with open(f\"{MANGA_REV_PATH}/reviews_{manga_id}.csv\", \"w\") as f1:\n",
        "            f1.write(f\"username,score,review,date\\n\")\n",
        "            for user, score, review, date in all_manga_reviews:\n",
        "                f1.write(\n",
        "                    f\"{user},{score},{review},{date}\\n\"\n",
        "                )\n",
        "\n",
        "    revised_manga_rev.add(manga_id)\n",
        "    with open(f\"{BASE_PATH}/_revised_manga_rev.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{manga_id}\\n\")"
      ],
      "metadata": {
        "id": "apafxOBJIFWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CREATING MONGO DB IMPORT FILES\n"
      ],
      "metadata": {
        "id": "Un3P4mNJeorw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining data previously retrieved in a format accepted by mongo import"
      ],
      "metadata": {
        "id": "8WBYJrJp27gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_userId(username, users):\n",
        "    return int(users[users[\" username\"] == username][\"user_id\"].values[0])\n",
        "\n",
        "def get_creation(username, users):\n",
        "    creation = datetime.fromisoformat(users[users[\" username\"] == username][\"joined_at\"].values[0]).replace(tzinfo=None)\n",
        "    return creation\n",
        "def gen_rand_date(start_date, end_date):\n",
        "    # Calcola la differenza in secondi tra le date\n",
        "    # Calcola il delta tra la data iniziale e quella finale\n",
        "    delta = end_date - start_date\n",
        "    # Genera un numero casuale di secondi tra l'intervallo\n",
        "    random_seconds = random.randint(0, int(delta.total_seconds()))\n",
        "    # Aggiungi i secondi casuali alla data iniziale\n",
        "    random_datetime = start_date + timedelta(seconds=random_seconds)\n",
        "    return random_datetime.isoformat()\n"
      ],
      "metadata": {
        "id": "ki0zJ8BBiITd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For MANGA Collection"
      ],
      "metadata": {
        "id": "kYwMWhuOevgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a file with all scores from users with no review, for each manga.\n",
        "To improve performance this file can be created when retrieving user lists and skipping the next two blocks"
      ],
      "metadata": {
        "id": "uEkGYYunK93E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f\"{MANGA_PATH}/manga_scores\"):\n",
        "          os.makedirs(f\"{MANGA_PATH}/manga_scores\", exist_ok=True)\n",
        "\n",
        "with open(f\"{BASE_PATH}/joined_users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    all_users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    all_users = [(int(x[0]), x[1]) for x in all_users] # Retrieving user_id and usernames to get reviwes only of saved users\n",
        "\n",
        "with open(f\"{MANGA_PATH}/manga_list.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    all_manga = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    all_manga = [(int(x[0]), x[1]) for x in all_manga] # Retrieving usernames to get reviwes only of saved manga\n",
        "\n",
        "if not os.path.exists(f\"{BASE_PATH}/_revised_manga_score.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_revised_manga_score.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "        pass\n",
        "\n",
        "with open(f\"{BASE_PATH}/_revised_manga_score.txt\", encoding=\"UTF-8\") as file:\n",
        "  revised_manga_score = set([int(x.strip()) for x in file.readlines()])\n",
        "\n",
        "len(revised_manga_score), len(all_users), len(all_manga)"
      ],
      "metadata": {
        "id": "ds3zXhK2ez8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipped = 0\n",
        "for i, (manga_id, manga_title) in enumerate(all_manga):\n",
        "    if manga_id in revised_manga_score:\n",
        "        skipped += 1\n",
        "        continue\n",
        "    scores = []\n",
        "    for j, (user_id, username) in enumerate(all_users):\n",
        "\n",
        "        print(f\"\\r manga:{i+1}/{len(all_manga)} x users:{j+1}/{len(all_users)}, skipped: {skipped}\", end=\"\")\n",
        "\n",
        "        user_file = f\"{USER_PATH}/user_{user_id}/user_{user_id}_manga_list.csv\"\n",
        "        if os.path.exists(user_file):\n",
        "            user_data = pd.read_csv(user_file)  # Colonne: manga_id, status, read_chapters, score\n",
        "            user_manga = user_data[user_data[\"manga_id\"] == manga_id]\n",
        "\n",
        "            if not user_manga.empty:\n",
        "                row = user_manga.iloc[0]  # Prendi la prima riga (se c'è più di una, è un'anomalia)\n",
        "                scores.append({\n",
        "                    \"user_id\": user_id,\n",
        "                    \"username\": username,\n",
        "                    \"status\": row[\"status\"],\n",
        "                    \"read_chapters\": row[\"read_chapters\"],\n",
        "                    \"score\": int(row[\"score\"])\n",
        "                })\n",
        "\n",
        "    if scores:\n",
        "        scores_df = pd.DataFrame(scores)\n",
        "        scores_df.to_csv(f\"{MANGA_SCORES_PATH}/manga_{manga_id}.csv\", index=False)\n",
        "\n",
        "    revised_manga_score.add(manga_id)\n",
        "    with open(f\"{BASE_PATH}/_revised_manga_score.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{manga_id}\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nUp903rkhGgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the file for mongo import**"
      ],
      "metadata": {
        "id": "cswyb3QL3t_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"_id\"\n",
        "\"name\"\n",
        "\"status\"\n",
        "\"chapters\"\n",
        "\"sumScores\"\n",
        "\"numScores\"\n",
        "\"genres\"\n",
        "\"type\"\n",
        "\"authors\"\n",
        "\"synopsis\"\n",
        "\"reviews\""
      ],
      "metadata": {
        "id": "hJvpOIhk6LPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_manga_files_with_pandas(manga_file, reviews_path, scores_path, output_file, users):\n",
        "\n",
        "    # Carica la lista dei manga\n",
        "    manga_df = pd.read_csv(manga_file)\n",
        "    manga_df[\"numScores\"] = 0\n",
        "    manga_df[\"sumScores\"] = 0\n",
        "    manga_df[\"reviews\"] = None  # Aggiungi colonna per le recensioni\n",
        "\n",
        "    # Itera sui manga per processare i file associati\n",
        "    for index, manga_row in manga_df.iterrows():\n",
        "        manga_id = manga_row[\"manga_id\"]\n",
        "        print(f\"\\r{index+1}/{len(manga_df)}, manga: {manga_id}\", end=\"\")\n",
        "\n",
        "        # Percorsi per i file delle recensioni e dei punteggi\n",
        "        reviews_file = os.path.join(reviews_path, f\"reviews_{manga_id}.csv\")\n",
        "        scores_file = os.path.join(scores_path, f\"manga_{manga_id}.csv\")\n",
        "\n",
        "        # Lista per raccogliere tutte le recensioni\n",
        "        reviews_list = []\n",
        "\n",
        "        # Carica recensioni con commenti (se esiste il file)\n",
        "        if os.path.exists(reviews_file):\n",
        "            reviews_df = pd.read_csv(reviews_file)\n",
        "            for _, review_row in reviews_df.iterrows():\n",
        "                reviews_list.append({\n",
        "                    \"userId\": get_userId(review_row[\"username\"], users),\n",
        "                    \"username\": review_row[\"username\"],\n",
        "                    \"comment\": review_row.get(\"review\", \"\"),\n",
        "                    \"score\": int(review_row[\"score\"]),\n",
        "                    \"timestamp\": review_row[\"date\"]\n",
        "                })\n",
        "                # Aggiorna i contatori\n",
        "                if int(review_row[\"score\"]) > 0:\n",
        "                    manga_df.at[index, \"numScores\"] += 1\n",
        "                    manga_df.at[index, \"sumScores\"] += int(review_row[\"score\"])\n",
        "\n",
        "        # Carica voti senza commenti (se esiste il file)\n",
        "        if os.path.exists(scores_file):\n",
        "            scores_df = pd.read_csv(scores_file)\n",
        "            for _, score_row in scores_df.iterrows():\n",
        "                if int(score_row[\"score\"]) > 0:\n",
        "                    reviews_list.append({\n",
        "                        \"userId\": score_row[\"user_id\"],\n",
        "                        \"username\": score_row[\"username\"],\n",
        "                        \"score\": int(score_row[\"score\"]),\n",
        "                        \"timestamp\": gen_rand_date(get_creation(score_row[\"username\"], users), datetime.now())\n",
        "                    })\n",
        "                    # Aggiorna i contatori\n",
        "                    manga_df.at[index, \"numScores\"] += 1\n",
        "                    manga_df.at[index, \"sumScores\"] += int(score_row[\"score\"])\n",
        "\n",
        "        # Salva le recensioni come JSON serializzato\n",
        "        manga_df.at[index, \"reviews\"] = json.dumps(reviews_list, ensure_ascii=False)\n",
        "\n",
        "    # Salva il risultato in un file CSV\n",
        "    manga_df.to_csv(output_file, index=False)\n",
        "    print(f\"File CSV creato: {output_file}\")\n",
        "\n",
        "# Percorsi dei file\n",
        "manga_file = MANGA_PATH + \"/manga_list.csv\"  # Il file CSV di input\n",
        "reviews_path = MANGA_REV_PATH       # Directory contenente i file delle recensioni\n",
        "scores_path = MANGA_SCORES_PATH        # Directory contenente i file dei punteggi\n",
        "output_file = MONGO_PATH + \"/manga_collection.csv\"  # Il file CSV di output\n",
        "\n",
        "users = pd.read_csv(f\"{BASE_PATH}/joined_users.csv\")\n",
        "\n",
        "\n",
        "process_manga_files_with_pandas(manga_file, reviews_path, scores_path, output_file, users)\n"
      ],
      "metadata": {
        "id": "9DNtmqe9cx3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the previous dataframe"
      ],
      "metadata": {
        "id": "k1RFs2P74BpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manga_coll = pd.read_csv(MONGO_PATH + \"/manga_collection.csv\")\n",
        "manga_coll.head()\n",
        "manga_coll.rename(columns={\"manga_id\": \"_id\", \"manga_title\": \"name\", \"media_type\" : \"type\", \"num_chapters\": \"chapters\"}, inplace=True)\n",
        "manga_coll.drop(columns=[\"start_date\", \"rank\", \"mean\"], inplace=True)\n",
        "try:\n",
        "    manga_coll['genres'] = manga_coll['genres'].apply(lambda x: str(x).replace(\" \", \"\").split(\"-\"))\n",
        "    manga_coll['authors'] = manga_coll['authors'].apply(lambda x: str(x).replace(\" \", \"\").replace(\"+\", \" \").split(\"-\"))\n",
        "    manga_coll['status'] = manga_coll['status'].apply(lambda x: 'COMPLETE' if x == \"finished\" else 'ONGOING')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "duplicati = manga_coll[manga_coll.duplicated(subset='_id', keep='first')]\n",
        "manga_coll_no_double =manga_coll.drop_duplicates(subset='_id', keep='first')\n",
        "# Mostra i risultati\n",
        "print(manga_coll.shape[0], duplicati.shape[0], manga_coll_no_double.shape[0])\n",
        "duplicati.head()\n",
        "manga_coll_no_double.to_csv(MONGO_PATH + \"/manga_collection.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "IJ3NqAvX7Wge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For ANIME Collection"
      ],
      "metadata": {
        "id": "NaVP9EDMAEY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve performance this file can be created when retrieving user lists and skipping the next two blocks"
      ],
      "metadata": {
        "id": "AGgpz9CZ4dhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f\"{ANIME_PATH}/anime_scores\"):\n",
        "          os.makedirs(f\"{ANIME_PATH}/anime_scores\", exist_ok=True)\n",
        "\n",
        "with open(f\"{BASE_PATH}/joined_users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    all_users = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    all_users = [(int(x[0]), x[1]) for x in all_users] # Retrieving user_id and usernames to get reviwes only of saved users\n",
        "\n",
        "with open(f\"{ANIME_PATH}/anime_list.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    file.readline()\n",
        "    all_anime = [x.strip().split(\",\") for x in file.readlines()]\n",
        "    all_anime = [(int(x[0]), x[1]) for x in all_anime] # Retrieving usernames to get reviwes only of saved anime\n",
        "\n",
        "if not os.path.exists(f\"{BASE_PATH}/_revised_anime_score.txt\"):\n",
        "    with open(f\"{BASE_PATH}/_revised_anime_score.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "        pass\n",
        "\n",
        "with open(f\"{BASE_PATH}/_revised_anime_score.txt\", encoding=\"UTF-8\") as file:\n",
        "  revised_anime_score = set([int(x.strip()) for x in file.readlines()])\n",
        "\n",
        "len(revised_anime_score), len(all_users), len(all_anime)"
      ],
      "metadata": {
        "id": "qIye0X5_ADDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipped = 0\n",
        "for i, (anime_id, anime_title) in enumerate(all_anime):\n",
        "    if anime_id in revised_anime_score:\n",
        "        skipped += 1\n",
        "        continue\n",
        "    scores = []\n",
        "    for j, (user_id, username) in enumerate(all_users):\n",
        "\n",
        "        print(f\"\\r anime:{i+1}/{len(all_anime)} x users:{j+1}/{len(all_users)}, skipped: {skipped}\", end=\"\")\n",
        "\n",
        "        user_file = f\"{USER_PATH}/user_{user_id}/user_{user_id}_anime_list.csv\"\n",
        "        if os.path.exists(user_file):\n",
        "            user_data = pd.read_csv(user_file)  # Colonne: anime_id, status, read_chapters, score\n",
        "            user_anime = user_data[user_data[\"anime_id\"] == anime_id]\n",
        "\n",
        "            if not user_anime.empty:\n",
        "                row = user_anime.iloc[0]  # Prendi la prima riga (se c'è più di una, è un'anomalia)\n",
        "                scores.append({\n",
        "                    \"userId\": user_id,\n",
        "                    \"username\": username,\n",
        "                    \"status\": row[\"status\"],\n",
        "                    \"watched_episodes\": row[\"watched_episodes\"],\n",
        "                    \"score\": int(row[\"score\"])\n",
        "                })\n",
        "\n",
        "    if scores:\n",
        "        scores_df = pd.DataFrame(scores)\n",
        "        scores_df.to_csv(f\"{ANIME_SCORES_PATH}/anime_{anime_id}.csv\", index=False)\n",
        "\n",
        "    revised_anime_score.add(anime_id)\n",
        "    with open(f\"{BASE_PATH}/_revised_anime_score.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
        "        file.write(f\"{anime_id}\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "noImlwq6AaWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the file for mongo import**"
      ],
      "metadata": {
        "id": "GWvYP48O4g7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"_id\"\n",
        "\"name\"\n",
        "\"status\"\n",
        "\"episodes\"\n",
        "\"sumScores\"\n",
        "\"numScores\"\n",
        "\"genres\"\n",
        "\"type\"\n",
        "\"source\"\n",
        "\"duration\"\n",
        "\"studio\"\n",
        "\"synopsis\"\n",
        "\"reviews\""
      ],
      "metadata": {
        "id": "nazUdLj_Bkco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_anime_files_with_pandas(anime_file, reviews_path, scores_path, output_file, users):\n",
        "\n",
        "    # Carica la lista dei anime\n",
        "    anime_df = pd.read_csv(anime_file)\n",
        "    anime_df[\"numScores\"] = 0\n",
        "    anime_df[\"sumScores\"] = 0\n",
        "    anime_df[\"reviews\"] = None  # Aggiungi colonna per le recensioni\n",
        "\n",
        "    # Itera sui anime per processare i file associati\n",
        "    for index, anime_row in anime_df.iterrows():\n",
        "        anime_id = anime_row[\"anime_id\"]\n",
        "        print(f\"\\r{index+1}/{len(anime_df)}, anime: {anime_id}\", end=\"\")\n",
        "\n",
        "        # Percorsi per i file delle recensioni e dei punteggi\n",
        "        reviews_file = os.path.join(reviews_path, f\"reviews_{anime_id}.csv\")\n",
        "        scores_file = os.path.join(scores_path, f\"anime_{anime_id}.csv\")\n",
        "\n",
        "        # Lista per raccogliere tutte le recensioni\n",
        "        reviews_list = []\n",
        "\n",
        "        # Carica recensioni con commenti (se esiste il file)\n",
        "        if os.path.exists(reviews_file):\n",
        "            reviews_df = pd.read_csv(reviews_file)\n",
        "            for _, review_row in reviews_df.iterrows():\n",
        "                reviews_list.append({\n",
        "                    \"userId\": get_userId(review_row[\"username\"], users),\n",
        "                    \"username\": review_row[\"username\"],\n",
        "                    \"comment\": review_row.get(\"review\", \"\"),\n",
        "                    \"score\": int(review_row[\"score\"]),\n",
        "                    \"timestamp\": review_row[\"date\"]\n",
        "                })\n",
        "                # Aggiorna i contatori\n",
        "                if int(review_row[\"score\"]) > 0:\n",
        "                    anime_df.at[index, \"numScores\"] += 1\n",
        "                    anime_df.at[index, \"sumScores\"] += int(review_row[\"score\"])\n",
        "\n",
        "        # Carica voti senza commenti (se esiste il file)\n",
        "        if os.path.exists(scores_file):\n",
        "            scores_df = pd.read_csv(scores_file)\n",
        "            for _, score_row in scores_df.iterrows():\n",
        "                if int(score_row[\"score\"]) > 0:\n",
        "                    reviews_list.append({\n",
        "                        \"userId\": score_row[\"userId\"],\n",
        "                        \"username\": score_row[\"username\"],\n",
        "                        \"score\": int(score_row[\"score\"]),\n",
        "                        \"timestamp\": gen_rand_date(get_creation(score_row[\"username\"], users), datetime.now())\n",
        "                    })\n",
        "                    # Aggiorna i contatori\n",
        "                    anime_df.at[index, \"numScores\"] += 1\n",
        "                    anime_df.at[index, \"sumScores\"] += int(score_row[\"score\"])\n",
        "\n",
        "        # Salva le recensioni come JSON serializzato\n",
        "        anime_df.at[index, \"reviews\"] = json.dumps(reviews_list, ensure_ascii=False)\n",
        "\n",
        "    # Salva il risultato in un file CSV\n",
        "    anime_df.to_csv(output_file, index=False)\n",
        "    print(f\"File CSV creato: {output_file}\")\n",
        "\n",
        "# Percorsi dei file\n",
        "anime_file = ANIME_PATH + \"/anime_list.csv\"  # Il file CSV di input\n",
        "reviews_path = ANIME_REV_PATH       # Directory contenente i file delle recensioni\n",
        "scores_path = ANIME_SCORES_PATH        # Directory contenente i file dei punteggi\n",
        "output_file = MONGO_PATH + \"/anime_collection.csv\"  # Il file CSV di output\n",
        "\n",
        "users = pd.read_csv(f\"{BASE_PATH}/joined_users.csv\")\n",
        "\n",
        "\n",
        "process_anime_files_with_pandas(anime_file, reviews_path, scores_path, output_file, users)\n"
      ],
      "metadata": {
        "id": "wkr91VTtBHr8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the previous dataframe"
      ],
      "metadata": {
        "id": "q7lZ7rHc4pR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anime_coll = pd.read_csv(MONGO_PATH + \"/anime_collection.csv\")\n",
        "anime_coll.head()\n",
        "anime_coll.rename(columns={\"anime_id\": \"_id\", \"anime_title\": \"name\", \" genres\": \"genres\", \"media_type\" : \"type\", \"num_episodes\": \"episodes\", \"average_ep_duration_sec\": \"duration\"}, inplace=True)\n",
        "anime_coll.drop(columns=[\"start_date\", \" end_date\", \"rank\", \"mean\"], inplace=True)\n",
        "try:\n",
        "    anime_coll['genres'] = anime_coll['genres'].apply(lambda x: str(x).replace(\" \", \"\").split(\"-\"))\n",
        "    anime_coll['studios'] = anime_coll['studios'].apply(lambda x: str(x).replace(\" \", \"\").replace(\"....\", \" \").split(\"-\") if x else None)\n",
        "    anime_coll['status'] = anime_coll['status'].apply(lambda x: 'COMPLETE' if x == \"finished_airing\" else 'ONGOING')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"errore: {e}\")\n",
        "\n",
        "duplicati = anime_coll[anime_coll.duplicated(subset='_id', keep='first')]\n",
        "anime_coll_no_double =anime_coll.drop_duplicates(subset='_id', keep='first')\n",
        "# Mostra i risultati\n",
        "print(anime_coll.shape[0], duplicati.shape[0], anime_coll_no_double.shape[0])\n",
        "duplicati.head()\n",
        "anime_coll_no_double.to_csv(MONGO_PATH + \"/anime_collection.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "KD1Vj8MIB865"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For USER collection"
      ],
      "metadata": {
        "id": "bHODl5qlVfbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"_id\"\n",
        "\"role\"\n",
        "\"username\"\n",
        "\"email\"\n",
        "\"password\"\n",
        "\"birthDate\"\n",
        "\"followers\" [1, 2, 4, 5…],\n",
        "\"privacyStatus\" {NOBODY, FOLLOWERS, ALL},\n",
        "\"createdAt\""
      ],
      "metadata": {
        "id": "DWOpZh-fzP5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_rand_privacy_status():\n",
        "    return random.choice([\"NOBODY\", \"FOLLOWERS\", \"ALL\"])"
      ],
      "metadata": {
        "id": "dFFOU-bqxPtv"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_users_files_with_pandas(users_file, output_file, users):\n",
        "\n",
        "    # Carica la lista dei users\n",
        "    users_df = pd.read_csv(users_file)\n",
        "    users_df[\"followers\"] = None\n",
        "    users_df[\"privacyStatus\"] = None\n",
        "    users_df[\"role\"] = None\n",
        "    users_df[\"email\"] = None\n",
        "    users_df[\"password\"] = None\n",
        "    users_df.rename(columns={\"user_id\": \"_id\", \"birthday\": \"birthdate\", \"joined_at\": \"createdAt\"}, inplace=True)\n",
        "    users_df.drop(columns=[\"location\"], inplace=True)\n",
        "\n",
        "\n",
        "    # Itera sui users per processare i file associati\n",
        "    for index, users_row in users_df.iterrows():\n",
        "        users_id = users_row[\"_id\"]\n",
        "        print(f\"\\r{index+1}/{len(users_df)}, users: {users_id}\", end=\"\")\n",
        "\n",
        "        # Lista per raccogliere tutte i followers\n",
        "        followers_list = []\n",
        "        num_follower = random.randint(0, 50)\n",
        "        follower_ids = users[\"user_id\"].sample(num_follower).tolist();\n",
        "        if(users_row[\"_id\"] not in follower_ids):\n",
        "            users_df.at[index, \"followers\"] = follower_ids\n",
        "        else:\n",
        "            follower_ids.remove(users_row[\"_id\"])\n",
        "            users_df.at[index, \"followers\"] = follower_ids\n",
        "\n",
        "        # aggiungo il privacy status\n",
        "        users_df.at[index, \"privacyStatus\"] = gen_rand_privacy_status()\n",
        "\n",
        "        # aggiungo il ruolo\n",
        "        users_df.at[index, \"role\"] = \"USER\"\n",
        "\n",
        "        # aggiungo email casuale\n",
        "        users_df.at[index, \"email\"] = f\"{users_row[' username']}@gmail.com\"\n",
        "\n",
        "        # aggiungo password casuale\n",
        "        users_df.at[index, \"password\"] = \"\"\n",
        "\n",
        "        # aggiungo compleanno casuale\n",
        "        if pd.isna(users_row[\"birthdate\"]):\n",
        "            start_date = get_creation(users_row[\" username\"], users) - timedelta(days=365*10)\n",
        "            users_df.at[index, \"birthdate\"] = gen_rand_date(start_date, get_creation(users_row[\" username\"], users))\n",
        "\n",
        "    # Salva il risultato in un file CSV\n",
        "    users_df.to_csv(output_file, index=False)\n",
        "    print(f\"File CSV creato: {output_file}\")\n",
        "\n",
        "# Percorsi dei file\n",
        "users_file = BASE_PATH + \"/joined_users.csv\"  # Il file CSV di input\n",
        "output_file = MONGO_PATH + \"/users_collection.csv\"  # Il file CSV di output\n",
        "\n",
        "users = pd.read_csv(f\"{BASE_PATH}/joined_users.csv\")\n",
        "\n",
        "\n",
        "process_users_files_with_pandas(users_file, output_file, users)\n"
      ],
      "metadata": {
        "id": "a2zyfhbHVxQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_coll= pd.read_csv(MONGO_PATH + \"/users_collection.csv\")\n",
        "\n",
        "duplicati = user_coll[user_coll.duplicated(subset='_id', keep='first')]\n",
        "user_coll_no_double =user_coll.drop_duplicates(subset='_id', keep='first')\n",
        "# Mostra i risultati\n",
        "print(user_coll.shape[0], duplicati.shape[0], user_coll_no_double.shape[0])"
      ],
      "metadata": {
        "id": "M6t7tzs2Z1ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UTILS"
      ],
      "metadata": {
        "id": "ymXnGimdcE64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When impporting csv into mongodb some data may not be formatted in the right way, usong the following script we can convert data types"
      ],
      "metadata": {
        "id": "MuvQ19WxwwY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert reviews.timestamp to date type and reviews.userId to string"
      ],
      "metadata": {
        "id": "khXUKkFVLG4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# db.myCollection.updateMany(\n",
        "#   {}, // Seleziona tutti i documenti. Puoi aggiungere filtri specifici se necessario.\n",
        "#   [\n",
        "#     {\n",
        "#       $set: {\n",
        "#         reviews: {\n",
        "#           $cond: {\n",
        "#             if: { $and: [ { $isArray: \"$reviews\" }, { $gt: [{ $size: \"$reviews\" }, 0] } ] },\n",
        "#             then: {\n",
        "#               $map: {\n",
        "#                 input: \"$reviews\",\n",
        "#                 as: \"review\",\n",
        "#                 in: {\n",
        "#                   $mergeObjects: [\n",
        "#                     \"$$review\",\n",
        "#                     { timestamp: { $toDate: \"$$review.timestamp\" },\n",
        "#                       userId: { $toString: \"$$review.userIs\"      }}\n",
        "#                   ]\n",
        "#                 }\n",
        "#               }\n",
        "#             },\n",
        "#             else: \"$reviews\" // Lascia invariato se non è un array o è un array vuoto.\n",
        "#           }\n",
        "#         }\n",
        "#       }\n",
        "#     }\n",
        "#   ]\n",
        "# );"
      ],
      "metadata": {
        "id": "cmhtxd0Q5Uz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for manga"
      ],
      "metadata": {
        "id": "hoiXjSaACfti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# db.manga.find({}).forEach(doc => {\n",
        "#     if (doc.genres && typeof doc.genres === \"string\") {\n",
        "#         db.manga.updateOne(\n",
        "#             { _id: doc._id },\n",
        "#             [{\n",
        "#                 $set: {\n",
        "#                     genres: {\n",
        "#                         $function: {\n",
        "#                             body: function (str) {\n",
        "#                                 try {\n",
        "#                                     return JSON.parse(str.replace(/'/g, '\"'));\n",
        "#                                 } catch (e) {\n",
        "#                                     return []; // Handle invalid JSON gracefully\n",
        "#                                 }\n",
        "#                             },\n",
        "#                             args: [\"$genres\"],\n",
        "#                             lang: \"js\"\n",
        "#                         }\n",
        "#                     }\n",
        "#                 }\n",
        "#             }]\n",
        "#         );\n",
        "#     }\n",
        "#\n",
        "#     if (doc.authors && typeof doc.authors === \"string\") {\n",
        "#         db.manga.updateOne(\n",
        "#             { _id: doc._id },\n",
        "#             [{\n",
        "#                 $set: {\n",
        "#                     authors: {\n",
        "#                         $function: {\n",
        "#                             body: function (str) {\n",
        "#                                 try {\n",
        "#                                     return JSON.parse(str.replace(/'/g, '\"'));\n",
        "#                                 } catch (e) {\n",
        "#                                     return []; // Handle invalid JSON gracefully\n",
        "#                                 }\n",
        "#                             },\n",
        "#                             args: [\"$authors\"],\n",
        "#                             lang: \"js\"\n",
        "#                         }\n",
        "#                     }\n",
        "#                 }\n",
        "#             }]\n",
        "#         );\n",
        "#     }\n",
        "# });"
      ],
      "metadata": {
        "id": "E8e14N7Kst3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for anime"
      ],
      "metadata": {
        "id": "PPbxuJh0ChXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# db.anime.find({}).forEach(doc => {\n",
        "#     if (doc.genres && typeof doc.genres === \"string\") {\n",
        "#         db.anime.updateOne(\n",
        "#             { _id: doc._id },\n",
        "#             [{\n",
        "#                 $set: {\n",
        "#                     genres: {\n",
        "#                         $function: {\n",
        "#                             body: function (str) {\n",
        "#                                 try {\n",
        "#                                     return JSON.parse(str.replace(/'/g, '\"'));\n",
        "#                                 } catch (e) {\n",
        "#                                     return []; // Handle invalid JSON gracefully\n",
        "#                                 }\n",
        "#                             },\n",
        "#                             args: [\"$genres\"],\n",
        "#                             lang: \"js\"\n",
        "#                         }\n",
        "#                     }\n",
        "#                 }\n",
        "#             }]\n",
        "#         );\n",
        "#     }\n",
        "#\n",
        "#     if (doc.studios && typeof doc.studios === \"string\") {\n",
        "#         db.anime.updateOne(\n",
        "#             { _id: doc._id },\n",
        "#             [{\n",
        "#                 $set: {\n",
        "#                     studios: {\n",
        "#                         $function: {\n",
        "#                             body: function (str) {\n",
        "#                                 try {\n",
        "#                                     return JSON.parse(str.replace(/'/g, '\"'));\n",
        "#                                 } catch (e) {\n",
        "#                                     return []; // Handle invalid JSON gracefully\n",
        "#                                 }\n",
        "#                             },\n",
        "#                             args: [\"$studios\"],\n",
        "#                             lang: \"js\"\n",
        "#                         }\n",
        "#                     }\n",
        "#                 }\n",
        "#             }]\n",
        "#         );\n",
        "#     }\n",
        "# });"
      ],
      "metadata": {
        "id": "k4zGQYGAtG2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# db.anime.updateMany(\n",
        "#     { studios: [\"nan\"] }, // Filtra i documenti con studios uguale a [\"nan\"]\n",
        "#     { $set: { studios: \"\" } } // Aggiorna il campo studios con una stringa vuota\n",
        "# );"
      ],
      "metadata": {
        "id": "zR5jIgnXEZH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING NODE4J IMPORTING FILES"
      ],
      "metadata": {
        "id": "BHs5jjkNCvZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Users nodes"
      ],
      "metadata": {
        "id": "DmuPAAA2HlXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "id:, username:, privacyStatus:,"
      ],
      "metadata": {
        "id": "U6EgJEk0I8TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_nodes = pd.read_csv(f\"{MONGO_PATH}/users_collection.csv\")\n",
        "user_nodes.rename(columns={\"_id\": \"id\", \" username\": \"username\"}, inplace=True)\n",
        "user_nodes.drop(columns=[\"birthdate\", \"createdAt\", \"followers\", \"role\", \"email\", \"password\"], inplace=True)\n",
        "user_nodes.head()\n",
        "\n",
        "if not os.path.exists(f\"{NEO_PATH}\"):\n",
        "    os.makedirs(f\"{NEO_PATH}\", exist_ok=True)\n",
        "\n",
        "user_nodes.to_csv(f\"{NEO_PATH}/user_nodes.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "SdJoeE20HWrd"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manga nodes"
      ],
      "metadata": {
        "id": "L6BsI66eOav4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "id:, name:, status:, chapters:, genres:[]"
      ],
      "metadata": {
        "id": "2IsdMCWLOdOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manga_nodes = pd.read_csv(f\"{MONGO_PATH}/manga_collection.csv\")\n",
        "manga_nodes.rename(columns={\"_id\": \"id\"}, inplace=True)\n",
        "manga_nodes.drop(columns=[\"type\", \"authors\", \"synopsis\", \"numScores\", \"sumScores\", \"reviews\"], inplace=True)\n",
        "manga_nodes.head()\n",
        "\n",
        "manga_nodes.to_csv(f\"{NEO_PATH}/manga_nodes.csv\", index=False)"
      ],
      "metadata": {
        "id": "Qa4iXswyOLbK"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Anime nodes"
      ],
      "metadata": {
        "id": "euBc_kTDfGEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "id:, name:, status:, episodes:, genres:[]"
      ],
      "metadata": {
        "id": "kwz72dnQfMg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anime_nodes = pd.read_csv(f\"{MONGO_PATH}/anime_collection.csv\")\n",
        "anime_nodes.rename(columns={\"_id\": \"id\"}, inplace=True)\n",
        "anime_nodes.drop(columns=[\"type\", \"studios\", \"synopsis\", \"numScores\", \"sumScores\", \"reviews\", \"duration\", \"source\"], inplace=True)\n",
        "anime_nodes.head()\n",
        "\n",
        "anime_nodes.to_csv(f\"{NEO_PATH}/anime_nodes.csv\", index=False)"
      ],
      "metadata": {
        "id": "qxwFA_6cfFyM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Follow Relationship"
      ],
      "metadata": {
        "id": "XuYAvUO4vbEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating relationship between users"
      ],
      "metadata": {
        "id": "-cOTGi_Q7rWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_users = pd.read_csv(f\"{MONGO_PATH}/users_collection.csv\")\n",
        "full_users.rename(columns={\"_id\": \"followed\", \" username\": \"username\", \"followers\": \"follower\"}, inplace=True)\n",
        "full_users.drop(columns=[\"birthdate\", \"createdAt\", \"role\", \"email\", \"password\", \"username\", \"privacyStatus\"], inplace=True)\n",
        "full_users[\"follower\"] = full_users[\"follower\"].apply(ast.literal_eval)\n",
        "relations = full_users.explode(\"follower\")\n",
        "relations = relations[[\"follower\", \"followed\"]]\n",
        "relations[\"relation\"] = \"FOLLOW\"\n",
        "relations = relations.dropna(substet = [\"follower\"])\n",
        "relations.shape[0]\n",
        "relations.to_csv(f\"{NEO_PATH}/follow_relationship.csv\", index=False)"
      ],
      "metadata": {
        "id": "D8t198Y_vfSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anime Relationship"
      ],
      "metadata": {
        "id": "FWU1xZKSd49o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating relationship between users and anime, adding progress attribute"
      ],
      "metadata": {
        "id": "woaRqyug8glL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_csv(f\"{BASE_PATH}/joined_users.csv\")\n",
        "users_ids= users[\"user_id\"].tolist()\n",
        "no_anime = 0\n",
        "list_anime_df = pd.DataFrame()\n",
        "for index, user_id in enumerate(users_ids):\n",
        "    print(f\"\\r{index+1}/{len(users)}, users: {user_id}, no_anime_list: {no_anime}\", end=\"\")\n",
        "    user_file = f\"{USER_PATH}/user_{user_id}/user_{user_id}_anime_list.csv\"\n",
        "    if os.path.exists(user_file):\n",
        "        user_anime_list = pd.read_csv(user_file)  # Colonne: anime_id, anime_title, status, watched_episodes, score\n",
        "        user_anime_list[\"userId\"] = user_id\n",
        "        user_anime_list.drop(columns=[\"status\", \"anime_title\", \"score\"], inplace=True)\n",
        "        user_anime_list.rename(columns={\"anime_id\": \"animeId\", \"watched_episodes\": \"progress\"}, inplace=True)\n",
        "        list_anime_df = pd.concat([list_anime_df, user_anime_list], ignore_index=True)\n",
        "    else:\n",
        "      no_anime += 1\n",
        "\n",
        "list_anime_df.shape"
      ],
      "metadata": {
        "id": "Ld7TkTSSd9HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if all anime in the relation have been retrieved"
      ],
      "metadata": {
        "id": "m70TMULx8wmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_anime = pd.read_csv(f\"{ANIME_PATH}/anime_list.csv\")\n",
        "anime_ids = all_anime[\"anime_id\"].tolist()\n",
        "filtered_anime_relation = list_anime_df[list_anime_df[\"animeId\"].isin(anime_ids)]\n",
        "list_anime_df.shape, filtered_anime_relation.shape"
      ],
      "metadata": {
        "id": "Y25ynAU_hy8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_anime_relation.to_csv(f\"{NEO_PATH}/anime_relationship.csv\", index=False)"
      ],
      "metadata": {
        "id": "5f_a0O8Xq_ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the file in chunks to import in neo4j"
      ],
      "metadata": {
        "id": "qFwWTRsw857M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_relations = pd.read_csv(f\"{NEO_PATH}/anime_relationship.csv\")\n",
        "splitted_df = np.array_split(all_relations, 13)\n",
        "final_shape = 0\n",
        "for i, df_part in enumerate(splitted_df):\n",
        "    final_shape += int(df_part.shape[0])\n",
        "    print(f\"shape {i}: {df_part.shape[0]}\")\n",
        "    df_part.to_csv(f\"{NEO_PATH}/anime_relationship_part_{i}.csv\", index=False)\n",
        "if final_shape == all_relations.shape[0]:\n",
        "    print(\"ok\")\n",
        "else:\n",
        "    print(final_size), print(all_relations.shape[0])"
      ],
      "metadata": {
        "id": "ZMd5NXxXRq8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manga Relationship"
      ],
      "metadata": {
        "id": "9RqBgmaYjoLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating relationship between users and manga, adding progress attribute"
      ],
      "metadata": {
        "id": "jCitnjOq9FTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_csv(f\"{BASE_PATH}/joined_users.csv\")\n",
        "users_ids= users[\"user_id\"].tolist()\n",
        "no_manga = 0\n",
        "list_manga_df = pd.DataFrame()\n",
        "for index, user_id in enumerate(users_ids):\n",
        "    print(f\"\\r{index+1}/{len(users)}, users: {user_id}, no_manga_list: {no_manga}\", end=\"\")\n",
        "    user_file = f\"{USER_PATH}/user_{user_id}/user_{user_id}_manga_list.csv\"\n",
        "    if os.path.exists(user_file):\n",
        "        user_manga_list = pd.read_csv(user_file)  # Colonne: manga_id, manga_title, status, watched_episodes, score\n",
        "        user_manga_list[\"userId\"] = user_id\n",
        "        user_manga_list.drop(columns=[\"status\", \"manga_title\", \"score\"], inplace=True)\n",
        "        user_manga_list.rename(columns={\"manga_id\": \"mangaId\", \"read_chapters\": \"progress\"}, inplace=True)\n",
        "        list_manga_df = pd.concat([list_manga_df, user_manga_list], ignore_index=True)\n",
        "    else:\n",
        "      no_manga += 1\n",
        "\n",
        "list_manga_df.shape\n",
        "list_manga_df.head()"
      ],
      "metadata": {
        "id": "S4u2-1w8jq_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if all manga in the relation have been retrieved"
      ],
      "metadata": {
        "id": "ycdC--5i9loP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_manga = pd.read_csv(f\"{MANGA_PATH}/manga_list.csv\")\n",
        "manga_ids = all_manga[\"manga_id\"].tolist()\n",
        "filtered_manga_relation = list_manga_df[list_manga_df[\"mangaId\"].isin(manga_ids)]\n",
        "list_manga_df.shape, filtered_manga_relation.shape"
      ],
      "metadata": {
        "id": "NBP3R6JKmCPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_manga_relation.to_csv(f\"{NEO_PATH}/manga_relationship.csv\", index=False)"
      ],
      "metadata": {
        "id": "df2Jwb-QmJl_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_relations = pd.read_csv(f\"{NEO_PATH}/manga_relationship.csv\")\n",
        "splitted_df = np.array_split(all_relations, 2)\n",
        "final_shape = 0\n",
        "for i, df_part in enumerate(splitted_df):\n",
        "    final_shape += int(df_part.shape[0])\n",
        "    print(f\"shape {i}: {df_part.shape[0]}\")\n",
        "    df_part.to_csv(f\"{NEO_PATH}/manga_relationship_part_{i}.csv\", index=False)\n",
        "if final_shape == all_relations.shape[0]:\n",
        "    print(\"ok\")\n",
        "else:\n",
        "    print(final_size), print(all_relations.shape[0])"
      ],
      "metadata": {
        "id": "_nP3ipdwcMIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utils"
      ],
      "metadata": {
        "id": "w5qo9UBfRo7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for user"
      ],
      "metadata": {
        "id": "rY3A0RJzRqsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD CSV WITH HEADERS FROM \"file:///user_nodes.csv\" AS row\n",
        "# MERGE (:User {id: toString(row.id), username: row.username, privacyStatus: row.privacyStatus});"
      ],
      "metadata": {
        "id": "fPDzChFbe2Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for manga"
      ],
      "metadata": {
        "id": "LVAvA0UaRszs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD CSV WITH HEADERS FROM \"file:///manga_nodes.csv\" AS row\n",
        "# WITH row, [x IN split(substring(replace(row.genres, \"'\",\"\\\"\"), 2, size(row.genres) - 4), \"\\\",\\\"\") | x] AS veroArray\n",
        "# MERGE (:Manga {id: toString(row.id), name: row.name, status: row.status, chapters: toInteger(row.chapters), genres: veroArray});"
      ],
      "metadata": {
        "id": "eLtmeBxFRuk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for anime"
      ],
      "metadata": {
        "id": "_WwNYxrHe4pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD CSV WITH HEADERS FROM \"file:///anime_nodes.csv\" AS row\n",
        "# WITH row, [x IN split(substring(replace(row.genres, \"'\",\"\\\"\"), 2, size(row.genres) - 4), \"\\\",\\\"\") | x] AS veroArray\n",
        "# MERGE (:Anime {id: toString(row.id), name: row.name, status: row.status, episodes: toInteger(row.episodes), genres: veroArray});"
      ],
      "metadata": {
        "id": "Wl87HO3ee8l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for follows relations"
      ],
      "metadata": {
        "id": "MYFNUlfVeLTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD CSV WITH HEADERS FROM \"file:///follows_relationship.csv\" AS row\n",
        "# MATCH (a:User {id: toString(row.follower)}), (b:User {id: toString(row.followed)})\n",
        "# MERGE (a)-[:FOLLOWS]->(b);"
      ],
      "metadata": {
        "id": "B6EtyTgQeNvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for anime relations"
      ],
      "metadata": {
        "id": "Ui6fdRqrnPr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# USING PERIODIC COMMIT 10000\n",
        "# LOAD CSV WITH HEADERS FROM \"file:///anime_relationship.csv\" AS row\n",
        "# MATCH (a:User {id: toString(row.userId)}), (b:Anime {id: toString(row.animeId)})\n",
        "# MERGE (a)-[:LIST_ELEMENT{progress: toInteger(row.progress)}]->(b);"
      ],
      "metadata": {
        "id": "6oW5SQ8HnN-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for manga relations"
      ],
      "metadata": {
        "id": "FLaDy0HLpo1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# USING PERIODIC COMMIT 1000\n",
        "# LOAD CSV WITH HEADERS FROM \"file:///manga_relationship.csv\" AS row\n",
        "# MATCH (a:User {id: toString(row.userId)}), (b:Manga {id: toString(row.mangaId)})\n",
        "# MERGE (a)-[:LIST_ELEMENT{progress: toInteger(row.progress)}]->(b);"
      ],
      "metadata": {
        "id": "WhAP0F0tprFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3Fe2Xa7bwTUx",
        "sDU_SHe_260H",
        "3KPZTcZGoCIC",
        "xBwSRINYJUbP",
        "2ZOTksf5HRkk",
        "kYwMWhuOevgw",
        "bHODl5qlVfbg",
        "ymXnGimdcE64",
        "DmuPAAA2HlXC",
        "L6BsI66eOav4",
        "euBc_kTDfGEH",
        "XuYAvUO4vbEf"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}